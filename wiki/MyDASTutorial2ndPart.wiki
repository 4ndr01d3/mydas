#summary Tutorial for MyDAS - Intermediate level
#labels Phase-Support
= MyDAS 1.6 Tutorial =

== Second Part: Developing your own Data Source ==
Now that you have been a real DAS source working on MyDAS lets get closer to a more interesting example: what if you have your data in an in-house format? and therefore the templates/examples don't apply for your data source?

Following this tutorial you will parse an in-house formated file, publish its information as a data source in MyDAS using the DAS command feature. You can used the IDE of your choice but the tutorial is done in such a way you can use just a terminal and a text editor.

=== Example File ===
The file of this example was built using information from the ensembl database homo_sapiens_core_56_37a. The file gives positional information of genes, transcripts and exons in the chromosome 5 organized in a separated by pipes `|` like this:
{{{
|chr|gene_id|gene_start|gene_end|trascript_id|transcript_start|transcript_end|exon_id|exon_start|exon_end|
}}}
You can see a sample of this file below.
{{{
|5|ENSG00000153404|140373|190087|ENST00000398036|140373|157131|ENSE00001648483|156888|157131|
|5|ENSG00000153404|140373|190087|ENST00000398036|140373|157131|ENSE00001135995|156186|156325|
|5|ENSG00000153404|140373|190087|ENST00000398036|140373|157131|ENSE00001136002|155460|155558|
|5|ENSG00000153404|140373|190087|ENST00000398036|140373|157131|ENSE00001136007|154990|155106|
|5|ENSG00000153404|140373|190087|ENST00000398036|140373|157131|ENSE00001136016|151628|151714|
|5|ENSG00000153404|140373|190087|ENST00000398036|140373|157131|ENSE00001136025|144942|145035|
|5|ENSG00000153404|140373|190087|ENST00000398036|140373|157131|ENSE00001136029|143495|143618|
}}}


=== Analyzing the Data ===
Firstly, we know from this file the information that our data source has to provide and from them that there will be just four types of features in this data source:
  * chromosome: In our example will be always the same(5) however we can create our source to support files with information from more than one chromosome. This type should look in DAS like: 
{{{ 
<TYPE id="Chromosome" cvId="SO:0000340">Chromosome</TYPE> 
}}}
  * genes: The file give us information about the start and end of the genes plus its ID. From there we have another type in our data source:
{{{
<TYPE id="Gene" cvId="SO:0000704">Gene</TYPE>
}}}
  But also we have the id field of each feature of this type and the tags start and end can be also deduced from there.
  * transcripts: As in the case of genes the information provided is the id, start and stop. So we will need a new DAS type:
{{{
<TYPE id="Transcript" cvId="SO:0000673">Transcript</TYPE>
}}}
  * exons: Same analysis as above:
{{{
<TYPE id="Exon" cvId="SO:0000147">Exon</TYPE>
}}}
Note that the numbers at cvId are valid Ontology Ids from the Sequence Ontology.

The other information that we have implicit in this file is that there is a hierarchy in those components. A chromosome contains genes, a gene contains transcripts, an transcript contains exons. 

For this demo we can assume some values for all the annotations, the method will be
{{{
<METHOD id="not_recorded" cvId="ECO:0000037">not_recorded</METHOD> 
}}}
And given that we don't know the score, phase and orientation, we will assume those values as not applicable.

=== Implementing the Data Source ===
Now lets go to work.  The task of implementing the data source can be divided in three parts:
  # Implement a parser of the file.
  # Implement one of the MyDAS Data Sources Interfaces.
  # Configure MyDAS with the new Data Source.

Something to consider before to start is the strategy to deal with the data. It means if  the file is gonna be parsed completely and keep it in memory as a model (As the gff example) or if the file will be processed every time something is requested.

Pros and cons? the whole file in memory will response quicker but the bigger the file, the most memory the system will require. Opposite situation with the other case; processing the file for each request doesn't require too much memory, but the response time will be proportional to the size of the file.

Maybe an intermediate approach with a preprocessing stage, and subsequents specialized processes per query, can be a better way, however it will require to focus in details that are not in the scope of this tutorial.

The approach to follow here will be parsing the file and keeping a model in memory. It is quite similar to the one used for the gff example, so you might one to have a look on the files on `[MyDasTemplate]/src/main/java/uk/ac/ebi/mydas/examples/` before to start coding.

So, lets start coding a parser for this file. The goal of the parser is to get the info loaded in memory using the model of MyDAS to represent segments an annotations.

==== Implement a parser of the file ====
  # Create a file in `[MyDasTemplate]/src/main/java/uk/ac/ebi/mydas/examples/` called `SeparatedByPipesParser.java` and opened to edit with the editor of your choice(vi, write, eclipse, intelliJ, etc.)
  # Add the empty class declaration wit his package and the classes to import. You can add those imports now or whenever they are required:
{{{
package uk.ac.ebi.mydas.examples;

import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Scanner;

import uk.ac.ebi.mydas.exceptions.DataSourceException;
import uk.ac.ebi.mydas.model.DasAnnotatedSegment;
import uk.ac.ebi.mydas.model.DasComponentFeature;
import uk.ac.ebi.mydas.model.DasFeature;
import uk.ac.ebi.mydas.model.DasMethod;
import uk.ac.ebi.mydas.model.DasType;

public class SeparatedByPipesParser {
}
}}}
  # Define the attributes of the class, which in this case are:
    * scanner: Object used to process the file line by line.
    * segments: List of the parsed segments.
    * types: List of the types used in this data source. From a previous analysis: chromosome, gene, transcript, exon. Attributes for each pf those types can be created to facilitate its use.
    * method: From the analysis of the file(above). we are going to use the same method for all the features.
    The code for this will be something like:
{{{
	private Scanner scanner;
	private ArrayList<DasAnnotatedSegment> segments;
	private ArrayList<DasType> types;
	private DasType chromosomeType,geneType,transcriptType,exonType;
	private DasMethod method;
}}}
  # Code the constructor of the class. It should receive an InputStream, instantiate the scanner with the stream, create the empty lists for types and segments and create the types and method to use through the source.
{{{
	public SeparatedByPipesParser(InputStream gffdoc) throws DataSourceException{
		scanner= new Scanner(gffdoc);
		segments= new ArrayList<DasAnnotatedSegment>();
		types= new ArrayList<DasType>();
		chromosomeType= new DasType("Chromosome", null, "SO:0000340", "Chromosome");
		geneType= new DasType("Gene", null, "SO:0000704", "Gene");
		transcriptType= new DasType("Transcript", null, "SO:0000673", "Transcript");
		exonType= new DasType("Exon", null, "SO:0000147", "Exon");
		types.add(chromosomeType);
		types.add(geneType);
		types.add(transcriptType);
		types.add(exonType);
		method = new DasMethod("not_recorded","not_recorded","ECO:0000037");
	}
}}}
  # Now we have to go though the whole file line by line. To do that you can create the next method:
{{{
	public Collection<DasAnnotatedSegment> parse() throws Exception{
		try {
			//first use a Scanner to get each line
			while ( scanner.hasNextLine() ){
				processLine( scanner.nextLine() );
			}
		} finally {
			//ensure the underlying stream is always closed
			scanner.close();
		}
		return this.segments;
	}
}}}
  # As you can see the previous code makes use of the method `processLine()` which we haven't created, so thats our next step. A line of this file is a a set of fields divided by pipes (`|`) as is described before, so we can process a line by splitting it by pipes. The acquired data is used then, to call the methods to get/create the segment(chromosome) and features(gene, transcript or exon).
{{{
	private void processLine(String aLine) throws Exception{
		String[] parts = aLine.split("\\|");
		if (parts.length<11)
			throw new Exception("Parsing Error: A line doesn't have the right number of fields ["+aLine+"]");
		DasAnnotatedSegment segment = this.getSegment(parts[1]);
		DasComponentFeature gene= this.getGene(parts[2],parts[3],parts[4],segment);
		DasComponentFeature transcript= this.getTranscript(parts[5],parts[6],parts[7],gene);
		this.getExon(parts[8],parts[9],parts[10],transcript);
	}
}}}
  # To get a segment we start by looking in the current list of segments in case this segment has been created already. If is found is returned. If not a new segment is created using the id of the chromosome. And then, added to the list and returned. the file is not giving us too much information about the chromosome, so we are using default values in most of its fields.
{{{
	private DasAnnotatedSegment getSegment(String segmentId) throws DataSourceException {
		for (DasAnnotatedSegment segment:segments)
			if (segment.getSegmentId().equals(segmentId))
				return segment;
		DasAnnotatedSegment newSegment = new DasAnnotatedSegment(segmentId,1,1,"FROM_PIPE_FILE",segmentId, new ArrayList<DasFeature>());
		segments.add(newSegment);
		return newSegment;
	}
}}} 
  # Now we need the methods to get gene, transcript and exon.The strategy is the same than the getSegment method, I mean we first look in the current feature for tis subcomponents, and if is found is returned, if not a new instance is created and added. The code for those 3 methods is:
{{{
	private DasComponentFeature getExon(	String exonID, 
						String start, 
						String stop, 
						DasComponentFeature transcript) 
	throws DataSourceException {
		for (DasComponentFeature feature:transcript.getReportableSubComponents())
			if(feature.getFeatureId().equals(exonID))
				return feature;
		int startI=0,stopI=0;
		try {
			startI=Integer.parseInt(start);
			stopI=Integer.parseInt(stop);
		}catch (NumberFormatException nfe){
			throw new DataSourceException("PARSE ERROR: The coordinates for the exon "+exonID+" should be numeric",nfe);
		}
		return transcript.addSubComponent(	exonID, 
							startI, 
							stopI, 
							startI, 
							stopI, 
							exonID, 
							this.exonType, 
							exonID, 
							exonID, 
							method, 
							null, 
							null, 
							null, 
							null, 
							null);
	}
	private DasComponentFeature getTranscript(	String transcriptID, 
							String start,  
							String stop,  
							DasComponentFeature gene)
 	throws DataSourceException {

		for (DasComponentFeature feature:gene.getReportableSubComponents())
			if(feature.getFeatureId().equals(transcriptID))
				return feature;
		int startI=0,stopI=0;
		try {
			startI=Integer.parseInt(start);
			stopI=Integer.parseInt(stop);
		}catch (NumberFormatException nfe){
			throw new DataSourceException("PARSE ERROR: The coordinates for the transcript "+transcriptID+" should be numeric",nfe);
		}
		return gene.addSubComponent(	transcriptID, 
						startI, 
						stopI, 
						startI, 
						stopI, 
						transcriptID, 
						this.transcriptType, 
						transcriptID, 
						transcriptID, 
						method, 
						null, 
						null, 
						null, 
						null, 
						null);
	}
	private DasComponentFeature getGene(	String geneID,
						String start,
						String stop, 
						DasAnnotatedSegment segment) 
	throws DataSourceException {
		for (DasFeature feature:segment.getFeatures())
			if(feature.getFeatureId().equals(geneID))
				return (DasComponentFeature)feature;
		int startI=0,stopI=0;
		try {
			startI=Integer.parseInt(start);
			stopI=Integer.parseInt(stop);
		}catch (NumberFormatException nfe){
			throw new DataSourceException("PARSE ERROR: The coordinates for the gene "+geneID+" should be numeric",nfe);
		}
		return segment.getSelfComponentFeature().addSubComponent(	geneID, 
										startI, 
										stopI, 
										startI,
										stopI, 
										geneID, 
										geneType, 
										geneID, 
										geneID, 
										method, 
										null, 
										null, 
										null, 
										null, 
										null);
	}
}}}

==== Implement one of the MyDAS Data Sources Interfaces ====
For this example we are going to implement the AnnotationDataSource, to start doing create a file in `[MyDasTemplate]/src/main/java/uk/ac/ebi/mydas/examples/` called `SeparatedByPipesDataSource.java`. The content of an empty class that implements this Interface is below, and it contains the classes to import, and the definition of the empty methods force to implement by that interface.
{{{
package uk.ac.ebi.mydas.examples;

import java.net.URL;
import java.util.Collection;
import java.util.Map;

import javax.servlet.ServletContext;

import uk.ac.ebi.mydas.configuration.DataSourceConfiguration;
import uk.ac.ebi.mydas.controller.CacheManager;
import uk.ac.ebi.mydas.datasource.AnnotationDataSource;
import uk.ac.ebi.mydas.exceptions.BadReferenceObjectException;
import uk.ac.ebi.mydas.exceptions.DataSourceException;
import uk.ac.ebi.mydas.exceptions.UnimplementedFeatureException;
import uk.ac.ebi.mydas.model.DasAnnotatedSegment;
import uk.ac.ebi.mydas.model.DasType;

public class SeparatedByPipesDataSource implements AnnotationDataSource {

	public void init(ServletContext servletContext,
			Map<String, String> globalParameters,
			DataSourceConfiguration dataSourceConfig)
			throws DataSourceException {
	}

	public void destroy() {
	}

	public DasAnnotatedSegment getFeatures(String segmentId, Integer maxbeans)
			throws BadReferenceObjectException, DataSourceException {
		return null;
	}

	public Collection<DasAnnotatedSegment> getFeatures(
			Collection<String> featureIdCollection, Integer maxbins)
			throws UnimplementedFeatureException, DataSourceException {
		return null;
	}

	public URL getLinkURL(String field, String id)
			throws UnimplementedFeatureException, DataSourceException {
		return null;
	}

	public Integer getTotalCountForType(DasType type)
			throws DataSourceException {
		return null;
	}

	public Collection<DasType> getTypes() throws DataSourceException {
		return null;
	}

	public void registerCacheManager(CacheManager cacheManager) {
	}

}

}}}
Now lets go from the top down, implementing all the methods of this Data Source.
  ** *init()*: This method is called when the data source is loaded, so it the goal of this method should be to prepare the data source to receive the queries; in our case it means to parse the file and keep the model in memory. We assume that the file to be loaded is gonna be the data source property `pipes_file` in the configuration file.
  The parameters of this method have to be save as attributes of this class(you have to create those attributes). The parser is now invoked and the model(segments and types) is also saved as parameter. Exceptions are catch and throw in case something goes wrong.
{{{
	public void init(ServletContext servletContext,
			Map<String, String> globalParameters,
			DataSourceConfiguration dataSourceConfig)
			throws DataSourceException {
		this.svCon = servletContext;
		this.globalParameters = globalParameters;
		this.config = dataSourceConfig;
		path = config.getDataSourceProperties().get("pipes_file");
		try {
			SeparatedByPipesParser parser = new SeparatedByPipesParser(new FileInputStream(servletContext.getRealPath(path)));
			segments = parser.parse();
			types = parser.getTypes();
		} catch (FileNotFoundException e) {
			throw new DataSourceException("The data source cannot be loaded. The file couldn't be oppened",e);
		} catch (Exception e) {
			throw new DataSourceException("The data source cannot be loaded because of parsing problems",e);
		}
	}
}}}
  ** *destroy()*: the goal of this method is to liberate the resources of the system whenever the data source is turned off. In our case there is not too much to do, maybe be sure the attributes of this class are null. which is not completely necessary cause thats something java is going to do later anyway, however in other cases this method can be important to close databases connections or network sockets.
{{{
	public void destroy() {
		this.svCon=null;
		this.globalParameters=null;
		this.config=null;
		this.path=null;
		this.segments=null;
		this.types=null;
		this.cacheManager = null;
	}
}}}
  ** *getFeatures()*: The first method to get Features, queries for the features of a segment id, giving that we already have an array of segments this method just requires to go though this array looking for the segment with the same id. In case the id is not in the array an exception should be throw it, to let !MyDAS report this in the appropriate way. 
  The attribute maxbeans is ignored for now, this implies that our data source is not implementing that capability.
{{{
	public DasAnnotatedSegment getFeatures(String segmentId, Integer maxbeans)
			throws BadReferenceObjectException, DataSourceException {
		for(DasAnnotatedSegment segment:segments){
			if (segment.getSegmentId().equals(segmentId))
				return segment;
		}
		throw new BadReferenceObjectException("The id is not in the file", segmentId);
	}
}}}
  ** *getFeatures()*: the second method to get features queries by the feature id.